# ğŸŒ¦ï¸ Weather ETL Pipeline  
### Endâ€‘toâ€‘End Extract â€¢ Transform â€¢ Load Project (Python + Pandas + SQLite)

This project demonstrates a complete, productionâ€‘style ETL pipeline built in Python.  
It extracts hourly weather data from a public API, transforms it into analyticsâ€‘ready tables, and loads it into a SQLite database for downstream analysis.

The goal of this project is to showcase practical data engineering skills, including:

- API extraction  
- Data cleaning and feature engineering  
- Building a modular ETL architecture  
- Loading structured data into a relational database  
- Designing transformations that reflect real business logic  

---

## ğŸš€ Project Overview

This pipeline retrieves **48 hours of hourly weather data**, performs a series of transformations to enrich and clean the dataset, and stores the final result in a SQLite database.

The project is structured using a clear, industryâ€‘standard ETL layout:
src/ extract/ transform/ load/

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Extract   â”‚
      â”‚  (API Call) â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚ Raw JSON
             â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚ Transform   â”‚
      â”‚ Pandas ETL  â”‚
      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚ Cleaned DataFrame
             â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚    Load     â”‚
      â”‚  SQLite DB  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


---

## ğŸ“¥ Extract Layer

The extract step calls a weather API and retrieves:

- Hourly temperature  
- Humidity  
- Wind speed & direction  
- Pressure  
- Visibility  
- Precipitation  
- Soil metrics  
- And more  

The raw response is stored in memory as a nested JSON structure.

---

## ğŸ”§ Transform Layer

The transform step uses **pandas** to convert the messy API response into a clean, tabular dataset.

### Key transformations include:

#### **Data Cleaning**
- Timestamp parsing and timezone normalization  
- Rounding numeric fields  
- Handling missing values  

#### **Feature Engineering**
- Rolling temperature averages  
- Rolling humidity averages  
- Temperature deltas  
- Day vs night classification  
- â€œFeels colder/warmerâ€ flags  
- Wind direction mapping (N, NE, E, etc.)  
- Wind risk index  
- Precipitation flags (rain/snow)  
- Comfort index  
- Visibility normalization  

These transformations mirror the kind of business logic used in analytics and forecasting.

---

## ğŸ—„ï¸ Load Layer

The final cleaned DataFrame is loaded into a **SQLite database** using Pythonâ€™s `sqlite3` module.

- Creates the database if it doesnâ€™t exist  
- Creates or replaces the target table  
- Loads all rows in a single operation  

The resulting database can be opened directly in **DB Browser for SQLite**.

---

## ğŸ“Š Example Use Cases

Once loaded, the dataset can support:

- Weather dashboards  
- Forecasting models  
- Risk scoring  
- Operational reporting  
- Exploratory analysis  

This project is designed to be extended â€” additional tables, aggregations, or downstream tools can be added easily.

---

## ğŸ—ï¸ How to Run the Pipeline

### **Manual Execution**

You can run each stage individually:
uv run -m src.extract.api uv run -m src.transform.transform uv run -m src.load.loader


Or run the full pipeline via:
uv run python src/main.py


---

## ğŸ•’ Automated Scheduling (Batch File + Windows Task Scheduler)

This project includes instructions for running the ETL pipeline automatically on a schedule â€” similar to how production pipelines operate.

### **1. Create a batch file**

Create a file named `run_weather_pipeline.bat`:
cd C:\Utilities\Repos\Weather-ETL uv run python src\main.py


- `cd` ensures the script runs from the correct working directory  
- `uv run` automatically uses the projectâ€™s virtual environment  
- No manual activation of `.venv` is required  

### **2. Schedule it with Windows Task Scheduler**

1. Open **Task Scheduler**  
2. Select **Create Basic Task**  
3. Name it (e.g., â€œWeather ETL Pipelineâ€)  
4. Choose a trigger (e.g., **Daily at 02:00**)  
5. Action â†’ **Start a Program**  
6. Program/script:  
C:\path\to\run_weather_pipeline.bat


7. Finish and save  

Your ETL pipeline will now run automatically every day.

This mirrors the scheduling approach used in real data engineering environments.

---

## ğŸ§ª Future Enhancements

- Add unit tests for transformation logic  
- Add daily and weekly aggregated tables  
- Add a scheduler (cron, Airflow, or Prefect)  
- Build a small dashboard (Streamlit or Power BI)  
- Add logging and error handling  

---

## ğŸ¯ Purpose of This Project

This ETL pipeline was built to demonstrate:

- Clean, modular Python engineering  
- Realâ€‘world data transformation skills  
- Ability to design and implement an endâ€‘toâ€‘end data workflow  
- Understanding of analyticsâ€‘ready data modeling  

Itâ€™s intentionally structured to reflect the expectations of data engineering teams.

---

## âš ï¸ Note on Environment Choices

Iâ€™m aware that SQLite and DB Browser are not typical enterpriseâ€‘grade solutions.  
This project was developed on a work laptop during quiet periods, which limited the tools I could install.

On my personal machine, I would normally use:

- PostgreSQL  
- A `.env` file for secrets  
- A containerized environment (Docker)  

For this demonstration, I chose a simple and effective setup to focus on showcasing my ability to design, build, and maintain a complete ETL pipeline from start to finish.

---

**Marcus Richardson**